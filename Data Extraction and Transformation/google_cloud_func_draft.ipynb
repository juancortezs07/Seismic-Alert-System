{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Japan API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pandas_read_xml as pdx\n",
    "from functools import reduce\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Extraction to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_get(dictionary, keys, default=None):\n",
    "    return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split(\".\"), dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdx.read_xml('http://service.iris.edu/fdsnws/event/1/query?includeallmagnitudes=TRUE&magnitudetype=all&minlatitude=30.180889170292048&minlongitude=129.20644141355123&maxlatitude=45.540717058168504&maxlongitude=146.68542509079882&starttime=2021-01-01T00:00:00&endtime=2022-01-01T00:00:00&format=xml')\n",
    "\n",
    "json_obj = df.to_json()\n",
    "\n",
    "json_format = json.loads(json_obj)\n",
    "\n",
    "data = pd.json_normalize(json_format)\n",
    "\n",
    "df_events = data['q:quakeml.0.eventParameters.event']\n",
    "\n",
    "events = []\n",
    "for entry in df_events:\n",
    "    for l in entry:\n",
    "        events.append(l)\n",
    "\n",
    "events_values = {'type' : [], \n",
    "                'place' : [], \n",
    "                'time' : [],\n",
    "                'author' : [],\n",
    "                'latitude' : [],\n",
    "                'longitude' : [],\n",
    "                'depth' : [],\n",
    "                'mag' : [],\n",
    "                'magType' : []}\n",
    "                \n",
    "for event in events:\n",
    "    events_values['type'].append(deep_get(event, 'type')) \n",
    "    events_values['place'].append(deep_get(event, 'description.text')) \n",
    "    events_values['time'].append(deep_get(event, 'origin.time.value')) \n",
    "    events_values['author'].append(deep_get(event, 'origin.creationInfo.author')) \n",
    "    events_values['latitude'].append(deep_get(event, 'origin.latitude.value')) \n",
    "    events_values['longitude'].append(deep_get(event, 'origin.longitude.value')) \n",
    "    events_values['depth'].append(deep_get(event, 'origin.depth.value')) \n",
    "    events_values['mag'].append(deep_get(event, 'magnitude.mag.value')) \n",
    "    events_values['magType'].append(deep_get(event, 'magnitude.type'))\n",
    "\n",
    "df_api_japan = pd.DataFrame(events_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Connection to cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_11_23_11_45_09'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client(project='Seismic Alert System')\n",
    "bucket = client.get_bucket('seismic-data-bucket')\n",
    "blob = bucket.blob(f'api_japan_{current_time}.csv')\n",
    "blob.upload_from_string(df.to_csv(index = False),content_type = 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Defining the final extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def japan_api_to_gcs():\n",
    "    def deep_get(dictionary, keys, default=None):\n",
    "        return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split(\".\"), dictionary)\n",
    "    \n",
    "    df = pdx.read_xml('http://service.iris.edu/fdsnws/event/1/query?includeallmagnitudes=TRUE&magnitudetype=all&minlatitude=30.180889170292048&minlongitude=129.20644141355123&maxlatitude=45.540717058168504&maxlongitude=146.68542509079882&starttime=2021-01-01T00:00:00&endtime=2022-01-01T00:00:00&format=xml')\n",
    "\n",
    "    json_obj = df.to_json()\n",
    "\n",
    "    json_format = json.loads(json_obj)\n",
    "\n",
    "    data = pd.json_normalize(json_format)\n",
    "\n",
    "    df_events = data['q:quakeml.0.eventParameters.event']\n",
    "\n",
    "    events = []\n",
    "    for entry in df_events:\n",
    "        for l in entry:\n",
    "            events.append(l)\n",
    "\n",
    "    events_values = {'type' : [], \n",
    "                    'place' : [], \n",
    "                    'time' : [],\n",
    "                    'author' : [],\n",
    "                    'latitude' : [],\n",
    "                    'longitude' : [],\n",
    "                    'depth' : [],\n",
    "                    'mag' : [],\n",
    "                    'magType' : []}\n",
    "                    \n",
    "    for event in events:\n",
    "        events_values['type'].append(deep_get(event, 'type')) \n",
    "        events_values['place'].append(deep_get(event, 'description.text')) \n",
    "        events_values['time'].append(deep_get(event, 'origin.time.value')) \n",
    "        events_values['author'].append(deep_get(event, 'origin.creationInfo.author')) \n",
    "        events_values['latitude'].append(deep_get(event, 'origin.latitude.value')) \n",
    "        events_values['longitude'].append(deep_get(event, 'origin.longitude.value')) \n",
    "        events_values['depth'].append(deep_get(event, 'origin.depth.value')) \n",
    "        events_values['mag'].append(deep_get(event, 'magnitude.mag.value')) \n",
    "        events_values['magType'].append(deep_get(event, 'magnitude.type'))\n",
    "\n",
    "    df_api_japan = pd.DataFrame(events_values)\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"APIs/\" + f'api_japan_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_japan.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "# -------------------------------------------------------Transformation------------------------------------------------------------------------------------------------\n",
    "\n",
    "    df_api_japan_t = df_api_japan\n",
    "    \n",
    "    df_api_japan_t['time'] = pd.to_datetime(df_api_japan_t['time'])#, unit='ns')\n",
    "\n",
    "    df_api_japan_t.latitude = df_api_japan_t.latitude.astype(float)\n",
    "    df_api_japan_t.longitude = df_api_japan_t.longitude.astype(float)\n",
    "    df_api_japan_t.depth = df_api_japan_t.depth.astype(float)\n",
    "    df_api_japan_t.mag = df_api_japan_t.mag.astype(float)\n",
    "\n",
    "    df_api_japan_t = df_api_japan_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "    df_api_japan_t.place = df_api_japan_t.place.str.title()\n",
    "\n",
    "    no_japan_location = df_api_japan_t[df_api_japan.place.str.contains('Japan|Kuril|Honshu') == False].index\n",
    "    df_api_japan_t.drop(no_japan_location, inplace=True)    \n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_japan_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_japan_t.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "\n",
    "def chile_api_to_cgs():\n",
    "    url = 'https://chilealerta.com/api/query/?user=demo&select=ultimos_sismos&limit=100&country=Chile'\n",
    "    json_obj = urlopen(url)\n",
    "    data = json.load(json_obj)\n",
    "    df_api_chile = pd.json_normalize(data, record_path=['ultimos_sismos_Chile'])\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"APIs/\" + f'api_chile_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_chile.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "    #-----------------------------------Transformation----------------------------------------------------------------------\n",
    "\n",
    "    df_api_chile_t = df_api_chile\n",
    "\n",
    "    df_api_chile_t.drop(columns=['state', 'local_time', 'chilean_time', 'id', 'url', 'source'], inplace=True)\n",
    "\n",
    "    df_api_chile_t.rename(columns={'utc_time' : 'time',\n",
    "                             'reference' : 'place',\n",
    "                             'magnitude' : 'mag',\n",
    "                             'scale' : 'magType'}, inplace=True)\n",
    "\n",
    "    df_api_chile_t.time = pd.to_datetime(df_api_chile_t.time)\n",
    "\n",
    "    df_api_chile_t = df_api_chile_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_chile_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_chile_t.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "def us_api_to_cgs():\n",
    "    url = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson'\n",
    "    json_obj = urlopen(url)\n",
    "    data = json.load(json_obj)\n",
    "\n",
    "    df_us_api = pd.json_normalize(data, record_path=['features'])\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"APIs/\" + f'api_us_{current_time}.csv')\n",
    "    blob.upload_from_string(df_us_api.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "    #-----------------------------------Transformation----------------------------------------------------------------------\n",
    "\n",
    "    df_api_us_t = df_us_api[['properties.time', 'properties.place', 'properties.mag', 'properties.magType', 'geometry.coordinates']]\n",
    "\n",
    "    df_api_us_t.rename(columns={'properties.time' : 'time',\n",
    "                          'properties.place' : 'place',\n",
    "                          'properties.mag' : 'mag',\n",
    "                          'properties.magType' : 'magType',\n",
    "                          'geometry.coordinates' : 'coordinates'}, inplace=True)\n",
    "\n",
    "    df_us_locations = pd.DataFrame(df_api_us_t[\"coordinates\"].to_list(), columns=['longitude', 'latitude', 'depth'])\n",
    "    df_api_us_t['latitude'] = df_us_locations.latitude\n",
    "    df_api_us_t['longitude'] = df_us_locations.longitude\n",
    "    df_api_us_t['depth'] = df_us_locations.depth\n",
    "    df_api_us_t.drop(columns='coordinates', inplace=True)\n",
    "\n",
    "    df_api_us_t.time = pd.to_datetime(df_api_us_t.time, unit='ms')\n",
    "\n",
    "    other_countries_locations = df_api_us_t[(((df_api_us_t.latitude < 20.911795455444313) & (df_api_us_t.latitude > 50.02924641916901)) & ((df_api_us_t.longitude < -124.65301770531302) & (df_api_us_t.longitude > -66.95789388605114))) | (df_api_us_t.place.str.contains('Japan|Russia|Canada|Mexico|Sea'))].index # I create the index refering to those entries\n",
    "    df_api_us_t.drop(other_countries_locations, inplace=True)\n",
    "\n",
    "    df_api_us_t = df_api_us_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_us_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_us_t.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "def main(data, context):\n",
    "    japan_api_to_gcs()\n",
    "    chile_api_to_cgs()\n",
    "    us_api_to_cgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Defining the transformations function trying this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_us_data():\n",
    "\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import io\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    source_bucket = storage_client.bucket(source_bucket)\n",
    "\n",
    "    col_names = ['type','id','properties.mag','properties.place','properties.time','properties.updated','properties.tz','properties.url',\n",
    "    'properties.detail','properties.felt','properties.cdi','properties.mmi','properties.alert','properties.status','properties.tsunami','properties.sig',\n",
    "    'properties.net','properties.code','properties.ids','properties.sources','properties.types','properties.nst','properties.dmin','properties.rms',\n",
    "    'properties.gap','properties.magType','properties.type','properties.title','geometry.type','geometry.coordinates']\n",
    "\n",
    "    df = pd.DataFrame(columns=col_names)\n",
    "\n",
    "    for file in list(source_bucket.list_blobs()):\n",
    "        file_path=\"gs://{}/{}\".format(file.bucket.name, file.name)\n",
    "        df = df.append(pd.read_csv(file_path, header=None, names=col_names))\n",
    "\n",
    "    df_api_us_t = df[['properties.time', 'properties.place', 'properties.mag', 'properties.magType', 'geometry.coordinates']]\n",
    "\n",
    "    df_api_us_t.rename(columns={'properties.time' : 'time',\n",
    "                          'properties.place' : 'place',\n",
    "                          'properties.mag' : 'mag',\n",
    "                          'properties.magType' : 'magType',\n",
    "                          'geometry.coordinates' : 'coordinates'}, inplace=True)\n",
    "\n",
    "    df_us_locations = pd.DataFrame(df_api_us_t[\"coordinates\"].to_list(), columns=['longitude', 'latitude', 'depth'])\n",
    "    df_api_us_t['latitude'] = df_us_locations.latitude\n",
    "    df_api_us_t['longitude'] = df_us_locations.longitude\n",
    "    df_api_us_t['depth'] = df_us_locations.depth\n",
    "    df_api_us_t.drop(columns='coordinates', inplace=True)\n",
    "\n",
    "    df_api_us_t.time = pd.to_datetime(df_api_us_t.time, unit='ms')\n",
    "\n",
    "    other_countries_locations = df_api_us_t[(((df_api_us_t.latitude < 20.911795455444313) & (df_api_us_t.latitude > 50.02924641916901)) & ((df_api_us_t.longitude < -124.65301770531302) & (df_api_us_t.longitude > -66.95789388605114))) | (df_api_us_t.place.str.contains('Japan|Russia|Canada|Mexico|Sea'))].index # I create the index refering to those entries\n",
    "    df_api_us_t.drop(other_countries_locations, inplace=True)\n",
    "\n",
    "    df_api_us_t = df_api_us_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_us_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_us_t.to_csv(index = False),content_type = 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_us_data():\n",
    "\n",
    "    import subprocess\n",
    "\n",
    "    result = subprocess.run(['gsutil', 'ls', 'seismic-data-bucket/US_API/*.csv'], stdout=subprocess.PIPE)\n",
    "\n",
    "    all_dat = pd.DataFrame()\n",
    "    for file in result.stdout.splitlines():\n",
    "        dat = pd.read_csv(file.strip())\n",
    "        all_dat = all_dat.append(dat, ignore_index=True)\n",
    "\n",
    "    df_api_us_t = all_dat[['properties.time', 'properties.place', 'properties.mag', 'properties.magType', 'geometry.coordinates']]\n",
    "\n",
    "    df_api_us_t.rename(columns={'properties.time' : 'time',\n",
    "                          'properties.place' : 'place',\n",
    "                          'properties.mag' : 'mag',\n",
    "                          'properties.magType' : 'magType',\n",
    "                          'geometry.coordinates' : 'coordinates'}, inplace=True)\n",
    "\n",
    "    df_us_locations = pd.DataFrame(df_api_us_t[\"coordinates\"].to_list(), columns=['longitude', 'latitude', 'depth'])\n",
    "    df_api_us_t['latitude'] = df_us_locations.latitude\n",
    "    df_api_us_t['longitude'] = df_us_locations.longitude\n",
    "    df_api_us_t['depth'] = df_us_locations.depth\n",
    "    df_api_us_t.drop(columns='coordinates', inplace=True)\n",
    "\n",
    "    df_api_us_t.time = pd.to_datetime(df_api_us_t.time, unit='ms')\n",
    "\n",
    "    other_countries_locations = df_api_us_t[(((df_api_us_t.latitude < 20.911795455444313) & (df_api_us_t.latitude > 50.02924641916901)) & ((df_api_us_t.longitude < -124.65301770531302) & (df_api_us_t.longitude > -66.95789388605114))) | (df_api_us_t.place.str.contains('Japan|Russia|Canada|Mexico|Sea'))].index # I create the index refering to those entries\n",
    "    df_api_us_t.drop(other_countries_locations, inplace=True)\n",
    "\n",
    "    df_api_us_t = df_api_us_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_us_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_us_t.to_csv(index = False),content_type = 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_us_data():\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket('seismic-data-bucket')\n",
    "    blobs = bucket.list_blobs(prefix='US_API/')\n",
    "\n",
    "    list_temp_raw = []\n",
    "    for file in blobs:\n",
    "        filename = file.name\n",
    "        temp = pd.read_csv('gs://' + 'seismic-data-bucket' + '/' + filename, encoding='utf-8')\n",
    "    list_temp_raw.append(temp)\n",
    "\n",
    "    df = pd.concat(list_temp_raw)\n",
    "\n",
    "    df_api_us_t = df[['properties.time', 'properties.place', 'properties.mag', 'properties.magType', 'geometry.coordinates']]\n",
    "\n",
    "    df_api_us_t.rename(columns={'properties.time' : 'time',\n",
    "                          'properties.place' : 'place',\n",
    "                          'properties.mag' : 'mag',\n",
    "                          'properties.magType' : 'magType',\n",
    "                          'geometry.coordinates' : 'coordinates'}, inplace=True)\n",
    "\n",
    "    df_us_locations = pd.DataFrame(df_api_us_t[\"coordinates\"].to_list(), columns=['longitude', 'latitude', 'depth'])\n",
    "    df_api_us_t['latitude'] = df_us_locations.latitude\n",
    "    df_api_us_t['longitude'] = df_us_locations.longitude\n",
    "    df_api_us_t['depth'] = df_us_locations.depth\n",
    "    df_api_us_t.drop(columns='coordinates', inplace=True)\n",
    "\n",
    "    df_api_us_t.time = pd.to_datetime(df_api_us_t.time, unit='ms')\n",
    "\n",
    "    other_countries_locations = df_api_us_t[(((df_api_us_t.latitude < 20.911795455444313) & (df_api_us_t.latitude > 50.02924641916901)) & ((df_api_us_t.longitude < -124.65301770531302) & (df_api_us_t.longitude > -66.95789388605114))) | (df_api_us_t.place.str.contains('Japan|Russia|Canada|Mexico|Sea'))].index # I create the index refering to those entries\n",
    "    df_api_us_t.drop(other_countries_locations, inplace=True)\n",
    "\n",
    "    df_api_us_t = df_api_us_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_us_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_us_t.to_csv(index = False),content_type = 'csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_data_transformation():\n",
    "    \n",
    "    df_api_us_t = df_api_us[['properties.time', 'properties.place', 'properties.mag', 'properties.magType', 'geometry.coordinates']]\n",
    "\n",
    "    df_api_us_t.rename(columns={'properties.time' : 'time',\n",
    "                          'properties.place' : 'place',\n",
    "                          'properties.mag' : 'mag',\n",
    "                          'properties.magType' : 'magType',\n",
    "                          'geometry.coordinates' : 'coordinates'}, inplace=True)\n",
    "\n",
    "    df_us_locations = pd.DataFrame(df_api_us_t[\"coordinates\"].to_list(), columns=['longitude', 'latitude', 'depth'])\n",
    "    df_api_us_t['latitude'] = df_us_locations.latitude\n",
    "    df_api_us_t['longitude'] = df_us_locations.longitude\n",
    "    df_api_us_t['depth'] = df_us_locations.depth\n",
    "    df_api_us_t.drop(columns='coordinates', inplace=True)\n",
    "\n",
    "    df_api_us_t.time = pd.to_datetime(df_api_us_t.time, unit='ms')\n",
    "\n",
    "    other_countries_locations = df_api_us_t[(((df_api_us_t.latitude < 20.911795455444313) & (df_api_us_t.latitude > 50.02924641916901)) & ((df_api_us_t.longitude < -124.65301770531302) & (df_api_us_t.longitude > -66.95789388605114))) | (df_api_us_t.place.str.contains('Japan|Russia|Canada|Mexico|Sea'))].index # I create the index refering to those entries\n",
    "    df_api_us_t.drop(other_countries_locations, inplace=True)\n",
    "\n",
    "    df_api_us_t = df_api_us_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "def japan_data_transformation():\n",
    "\n",
    "    df_api_japan_t = df_api_japan\n",
    "    \n",
    "    df_api_japan_t['time'] = pd.to_datetime(df_api_japan_t['time'])#, unit='ns')\n",
    "\n",
    "    df_api_japan_t.latitude = df_api_japan_t.latitude.astype(float)\n",
    "    df_api_japan_t.longitude = df_api_japan_t.longitude.astype(float)\n",
    "    df_api_japan_t.depth = df_api_japan_t.depth.astype(float)\n",
    "    df_api_japan_t.mag = df_api_japan_t.mag.astype(float)\n",
    "\n",
    "    df_api_japan_t = df_api_japan_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "    df_api_japan_t.place = df_api_japan_t.place.str.title()\n",
    "\n",
    "    no_japan_location = df_api_japan_t[df_api_japan.place.str.contains('Japan|Kuril|Honshu') == False].index\n",
    "    df_api_japan_t.drop(no_japan_location, inplace=True)\n",
    "\n",
    "\n",
    "def chile_data_transformation():\n",
    "    df_api_chile_t = df_api_chile\n",
    "\n",
    "    df_api_chile_t.drop(columns=['state', 'local_time', 'chilean_time', 'id', 'url', 'source'], inplace=True)\n",
    "\n",
    "    df_api_chile_t.rename(columns={'utc_time' : 'time',\n",
    "                             'reference' : 'place',\n",
    "                             'magnitude' : 'mag',\n",
    "                             'scale' : 'magType'}, inplace=True)\n",
    "\n",
    "    df_api_chile_t.time = pd.to_datetime(df_api_chile_t.time)\n",
    "\n",
    "    df_api_chile_t = df_api_chile_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "\n",
    "def main(data, context):\n",
    "    us_data_transformation()\n",
    "    japan_data_transformation()\n",
    "    chile_data_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_request non-retriable exception: Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object. Permission 'storage.objects.get' denied on resource (or it may not exist)., 401\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\retry.py\", line 115, in retry_request\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\core.py\", line 415, in _request\n",
      "    validate_response(status, contents, path, args)\n",
      "  File \"c:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\retry.py\", line 102, in validate_response\n",
      "    raise HttpError(error)\n",
      "gcsfs.retry.HttpError: Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object. Permission 'storage.objects.get' denied on resource (or it may not exist)., 401\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object. Permission 'storage.objects.get' denied on resource (or it may not exist)., 401",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mgs://seismic-data-bucket/APIs/api_japan_2022_11_23_16_02_14.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1730\u001b[0m     f,\n\u001b[0;32m   1731\u001b[0m     mode,\n\u001b[0;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1738\u001b[0m )\n\u001b[0;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:714\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    711\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    713\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    715\u001b[0m     path_or_buf,\n\u001b[0;32m    716\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    717\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    718\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    719\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    720\u001b[0m )\n\u001b[0;32m    722\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    723\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:412\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     file_obj \u001b[39m=\u001b[39m fsspec\u001b[39m.\u001b[39;49mopen(\n\u001b[0;32m    411\u001b[0m         filepath_or_buffer, mode\u001b[39m=\u001b[39;49mfsspec_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(storage_options \u001b[39mor\u001b[39;49;00m {})\n\u001b[1;32m--> 412\u001b[0m     )\u001b[39m.\u001b[39;49mopen()\n\u001b[0;32m    413\u001b[0m \u001b[39m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mtuple\u001b[39m(err_types_to_retry_with_anon):\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fsspec\\core.py:135\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    129\u001b[0m     \u001b[39m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[0;32m    131\u001b[0m \u001b[39m    The OpenFile object should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m    instances persisting. You must, therefore, keep a reference to the OpenFile\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    during the life of the file-like it generates.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__enter__\u001b[39;49m()\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fsspec\\core.py:103\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    101\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, mode\u001b[39m=\u001b[39;49mmode)\n\u001b[0;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfobjects \u001b[39m=\u001b[39m [f]\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fsspec\\spec.py:1106\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     ac \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mautocommit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_intrans)\n\u001b[1;32m-> 1106\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_open(\n\u001b[0;32m   1107\u001b[0m         path,\n\u001b[0;32m   1108\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m   1109\u001b[0m         block_size\u001b[39m=\u001b[39mblock_size,\n\u001b[0;32m   1110\u001b[0m         autocommit\u001b[39m=\u001b[39mac,\n\u001b[0;32m   1111\u001b[0m         cache_options\u001b[39m=\u001b[39mcache_options,\n\u001b[0;32m   1112\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1113\u001b[0m     )\n\u001b[0;32m   1114\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1115\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mfsspec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m compr\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\core.py:1291\u001b[0m, in \u001b[0;36mGCSFileSystem._open\u001b[1;34m(self, path, mode, block_size, cache_options, acl, consistency, metadata, autocommit, fixed_key_metadata, generation, **kwargs)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     block_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_block_size\n\u001b[0;32m   1290\u001b[0m const \u001b[39m=\u001b[39m consistency \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconsistency\n\u001b[1;32m-> 1291\u001b[0m \u001b[39mreturn\u001b[39;00m GCSFile(\n\u001b[0;32m   1292\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1293\u001b[0m     path,\n\u001b[0;32m   1294\u001b[0m     mode,\n\u001b[0;32m   1295\u001b[0m     block_size,\n\u001b[0;32m   1296\u001b[0m     cache_options\u001b[39m=\u001b[39mcache_options,\n\u001b[0;32m   1297\u001b[0m     consistency\u001b[39m=\u001b[39mconst,\n\u001b[0;32m   1298\u001b[0m     metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[0;32m   1299\u001b[0m     acl\u001b[39m=\u001b[39macl,\n\u001b[0;32m   1300\u001b[0m     autocommit\u001b[39m=\u001b[39mautocommit,\n\u001b[0;32m   1301\u001b[0m     fixed_key_metadata\u001b[39m=\u001b[39mfixed_key_metadata,\n\u001b[0;32m   1302\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1303\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\core.py:1448\u001b[0m, in \u001b[0;36mGCSFile.__init__\u001b[1;34m(self, gcsfs, path, mode, block_size, autocommit, cache_type, cache_options, acl, consistency, metadata, content_type, timeout, fixed_key_metadata, generation, **kwargs)\u001b[0m\n\u001b[0;32m   1446\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttempt to open a bucket\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration \u001b[39m=\u001b[39m _coalesce_generation(generation, path_generation)\n\u001b[1;32m-> 1448\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m   1449\u001b[0m     gcsfs,\n\u001b[0;32m   1450\u001b[0m     path,\n\u001b[0;32m   1451\u001b[0m     mode,\n\u001b[0;32m   1452\u001b[0m     block_size,\n\u001b[0;32m   1453\u001b[0m     autocommit\u001b[39m=\u001b[39mautocommit,\n\u001b[0;32m   1454\u001b[0m     cache_type\u001b[39m=\u001b[39mcache_type,\n\u001b[0;32m   1455\u001b[0m     cache_options\u001b[39m=\u001b[39mcache_options,\n\u001b[0;32m   1456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1457\u001b[0m )\n\u001b[0;32m   1458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcsfs \u001b[39m=\u001b[39m gcsfs\n\u001b[0;32m   1459\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbucket \u001b[39m=\u001b[39m bucket\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fsspec\\spec.py:1462\u001b[0m, in \u001b[0;36mAbstractBufferedFile.__init__\u001b[1;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[0;32m   1460\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m=\u001b[39m size\n\u001b[0;32m   1461\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1462\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetails[\u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m caches[cache_type](\n\u001b[0;32m   1464\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_range, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcache_options\n\u001b[0;32m   1465\u001b[0m     )\n\u001b[0;32m   1466\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\core.py:1484\u001b[0m, in \u001b[0;36mGCSFile.details\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   1482\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetails\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1483\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_details \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1484\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs\u001b[39m.\u001b[39;49minfo(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, generation\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration)\n\u001b[0;32m   1485\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_details\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fsspec\\asyn.py:113\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    112\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m obj \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloop, func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fsspec\\asyn.py:98\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m FSTimeoutError \u001b[39mfrom\u001b[39;00m \u001b[39mreturn_result\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(return_result, \u001b[39mBaseException\u001b[39;00m):\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mraise\u001b[39;00m return_result\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m return_result\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fsspec\\asyn.py:53\u001b[0m, in \u001b[0;36m_runner\u001b[1;34m(event, coro, result, timeout)\u001b[0m\n\u001b[0;32m     51\u001b[0m     coro \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mwait_for(coro, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     result[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m coro\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m     55\u001b[0m     result[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m ex\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\core.py:782\u001b[0m, in \u001b[0;36mGCSFileSystem._info\u001b[1;34m(self, path, generation, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39m# Check exact file path\u001b[39;00m\n\u001b[0;32m    781\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 782\u001b[0m     exact \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_object(path)\n\u001b[0;32m    783\u001b[0m     \u001b[39m# this condition finds a \"placeholder\" - still need to check if it's a directory\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m exact[\u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m exact[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\core.py:495\u001b[0m, in \u001b[0;36mGCSFileSystem._get_object\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39m# Work around various permission settings. Prefer an object get (storage.objects.get), but\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[39m# fall back to a bucket list + filter to object name (storage.objects.list).\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 495\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\n\u001b[0;32m    496\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mb/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/o/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m, bucket, key, json_out\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, generation\u001b[39m=\u001b[39mgeneration\n\u001b[0;32m    497\u001b[0m     )\n\u001b[0;32m    498\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    499\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mForbidden\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\core.py:422\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[1;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m    419\u001b[0m     \u001b[39mself\u001b[39m, method, path, \u001b[39m*\u001b[39margs, json_out\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, info_out\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    420\u001b[0m ):\n\u001b[0;32m    421\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m.\u001b[39mupper()\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 422\u001b[0m     status, headers, info, contents \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[0;32m    423\u001b[0m         method, path, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    424\u001b[0m     )\n\u001b[0;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m json_out:\n\u001b[0;32m    426\u001b[0m         \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(contents)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    220\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m caller(func, \u001b[39m*\u001b[39m(extras \u001b[39m+\u001b[39m args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\retry.py:152\u001b[0m, in \u001b[0;36mretry_request\u001b[1;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    151\u001b[0m logger\u001b[39m.\u001b[39mexception(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m non-retriable exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[1;32m--> 152\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\retry.py:115\u001b[0m, in \u001b[0;36mretry_request\u001b[1;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m retry \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    114\u001b[0m         \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39msleep(\u001b[39mmin\u001b[39m(random\u001b[39m.\u001b[39mrandom() \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (retry \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m32\u001b[39m))\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[0;32m    117\u001b[0m     HttpError,\n\u001b[0;32m    118\u001b[0m     requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     aiohttp\u001b[39m.\u001b[39mclient_exceptions\u001b[39m.\u001b[39mClientError,\n\u001b[0;32m    122\u001b[0m ) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    124\u001b[0m         \u001b[39misinstance\u001b[39m(e, HttpError)\n\u001b[0;32m    125\u001b[0m         \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39mcode \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m\n\u001b[0;32m    126\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequester pays\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39mmessage\n\u001b[0;32m    127\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\core.py:415\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[1;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m info \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mrequest_info  \u001b[39m# for debug only\u001b[39;00m\n\u001b[0;32m    413\u001b[0m contents \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m r\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 415\u001b[0m validate_response(status, contents, path, args)\n\u001b[0;32m    416\u001b[0m \u001b[39mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[1;32mc:\\Users\\Auli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gcsfs\\retry.py:102\u001b[0m, in \u001b[0;36mvalidate_response\u001b[1;34m(status, content, path, args)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBad Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (path, msg))\n\u001b[0;32m    101\u001b[0m \u001b[39melif\u001b[39;00m error:\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mraise\u001b[39;00m HttpError(error)\n\u001b[0;32m    103\u001b[0m \u001b[39melif\u001b[39;00m status:\n\u001b[0;32m    104\u001b[0m     \u001b[39mraise\u001b[39;00m HttpError({\u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m: status, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m: msg})  \u001b[39m# text-like\u001b[39;00m\n",
      "\u001b[1;31mHttpError\u001b[0m: Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object. Permission 'storage.objects.get' denied on resource (or it may not exist)., 401"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('gs://seismic-data-bucket/APIs/api_japan_2022_11_23_16_02_14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google function backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pandas_read_xml as pdx\n",
    "from functools import reduce\n",
    "from google.cloud import storage\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def japan_api_to_gcs():\n",
    "    def deep_get(dictionary, keys, default=None):\n",
    "        return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split(\".\"), dictionary)\n",
    "    \n",
    "    df = pdx.read_xml('http://service.iris.edu/fdsnws/event/1/query?includeallmagnitudes=TRUE&magnitudetype=all&minlatitude=30.180889170292048&minlongitude=129.20644141355123&maxlatitude=45.540717058168504&maxlongitude=146.68542509079882&starttime=2021-01-01T00:00:00&endtime=2022-01-01T00:00:00&format=xml')\n",
    "\n",
    "    json_obj = df.to_json()\n",
    "\n",
    "    json_format = json.loads(json_obj)\n",
    "\n",
    "    data = pd.json_normalize(json_format)\n",
    "\n",
    "    df_events = data['q:quakeml.0.eventParameters.event']\n",
    "\n",
    "    events = []\n",
    "    for entry in df_events:\n",
    "        for l in entry:\n",
    "            events.append(l)\n",
    "\n",
    "    events_values = {'type' : [], \n",
    "                    'place' : [], \n",
    "                    'time' : [],\n",
    "                    'author' : [],\n",
    "                    'latitude' : [],\n",
    "                    'longitude' : [],\n",
    "                    'depth' : [],\n",
    "                    'mag' : [],\n",
    "                    'magType' : []}\n",
    "                    \n",
    "    for event in events:\n",
    "        events_values['type'].append(deep_get(event, 'type')) \n",
    "        events_values['place'].append(deep_get(event, 'description.text')) \n",
    "        events_values['time'].append(deep_get(event, 'origin.time.value')) \n",
    "        events_values['author'].append(deep_get(event, 'origin.creationInfo.author')) \n",
    "        events_values['latitude'].append(deep_get(event, 'origin.latitude.value')) \n",
    "        events_values['longitude'].append(deep_get(event, 'origin.longitude.value')) \n",
    "        events_values['depth'].append(deep_get(event, 'origin.depth.value')) \n",
    "        events_values['mag'].append(deep_get(event, 'magnitude.mag.value')) \n",
    "        events_values['magType'].append(deep_get(event, 'magnitude.type'))\n",
    "\n",
    "    df_api_japan = pd.DataFrame(events_values)\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"APIs/\" + f'api_japan_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_japan.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "def chile_api_to_cgs():\n",
    "    url = 'https://chilealerta.com/api/query/?user=demo&select=ultimos_sismos&limit=100&country=Chile'\n",
    "    json_obj = urlopen(url)\n",
    "    data = json.load(json_obj)\n",
    "    df_api_chile = pd.json_normalize(data, record_path=['ultimos_sismos_Chile'])\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"APIs/\" + f'api_chile_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_chile.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "def us_api_to_cgs():\n",
    "    url = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson'\n",
    "    json_obj = urlopen(url)\n",
    "    data = json.load(json_obj)\n",
    "\n",
    "    df_us_api = pd.json_normalize(data, record_path=['features'])\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"US_API/\" + f'api_us_{current_time}.csv')\n",
    "    blob.upload_from_string(df_us_api.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "\n",
    "\n",
    "def main(data, context):\n",
    "    japan_api_to_gcs()\n",
    "    chile_api_to_cgs()\n",
    "    us_api_to_cgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('apis datasets/api_us_20221123_111700.csv')\n",
    "data2 = pd.read_csv('apis datasets/api_japan_20221123_111700.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>place</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>depth</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 14:31:34.776</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.3</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>31.1148</td>\n",
       "      <td>142.2999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-31 14:23:50.014</td>\n",
       "      <td>Kyushu, Japan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>mb</td>\n",
       "      <td>27260.0</td>\n",
       "      <td>32.1049</td>\n",
       "      <td>130.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-30 23:22:38.756</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.2</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>30.9772</td>\n",
       "      <td>142.3701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-30 19:19:16.109</td>\n",
       "      <td>Off East Coast Of Honshu, Japan</td>\n",
       "      <td>4.5</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>33.1948</td>\n",
       "      <td>142.5877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-30 06:08:44.798</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.3</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>30.9180</td>\n",
       "      <td>141.8623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2021-01-05 21:02:41.764</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.1</td>\n",
       "      <td>mb</td>\n",
       "      <td>92210.0</td>\n",
       "      <td>33.0845</td>\n",
       "      <td>140.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2021-01-05 02:16:21.501</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.5</td>\n",
       "      <td>mb</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>32.3940</td>\n",
       "      <td>141.2047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2021-01-03 14:14:54.692</td>\n",
       "      <td>Eastern Sea Of Japan</td>\n",
       "      <td>4.1</td>\n",
       "      <td>mb</td>\n",
       "      <td>250430.0</td>\n",
       "      <td>43.6306</td>\n",
       "      <td>138.7964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2021-01-02 00:45:00.272</td>\n",
       "      <td>Off East Coast Of Honshu, Japan</td>\n",
       "      <td>4.6</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>33.4333</td>\n",
       "      <td>141.4589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>2021-01-01 01:50:51.107</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.4</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>30.6110</td>\n",
       "      <td>142.0702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>705 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time                            place  mag magType  \\\n",
       "0    2021-12-31 14:31:34.776       Southeast Of Honshu, Japan  4.3      mb   \n",
       "1    2021-12-31 14:23:50.014                    Kyushu, Japan  4.0      mb   \n",
       "2    2021-12-30 23:22:38.756       Southeast Of Honshu, Japan  4.2      mb   \n",
       "3    2021-12-30 19:19:16.109  Off East Coast Of Honshu, Japan  4.5      mb   \n",
       "4    2021-12-30 06:08:44.798       Southeast Of Honshu, Japan  4.3      mb   \n",
       "..                       ...                              ...  ...     ...   \n",
       "700  2021-01-05 21:02:41.764       Southeast Of Honshu, Japan  4.1      mb   \n",
       "701  2021-01-05 02:16:21.501       Southeast Of Honshu, Japan  4.5      mb   \n",
       "702  2021-01-03 14:14:54.692             Eastern Sea Of Japan  4.1      mb   \n",
       "703  2021-01-02 00:45:00.272  Off East Coast Of Honshu, Japan  4.6      mb   \n",
       "704  2021-01-01 01:50:51.107       Southeast Of Honshu, Japan  4.4      mb   \n",
       "\n",
       "        depth  latitude  longitude  \n",
       "0     10000.0   31.1148   142.2999  \n",
       "1     27260.0   32.1049   130.0111  \n",
       "2     10000.0   30.9772   142.3701  \n",
       "3     10000.0   33.1948   142.5877  \n",
       "4     10000.0   30.9180   141.8623  \n",
       "..        ...       ...        ...  \n",
       "700   92210.0   33.0845   140.1500  \n",
       "701   35000.0   32.3940   141.2047  \n",
       "702  250430.0   43.6306   138.7964  \n",
       "703   10000.0   33.4333   141.4589  \n",
       "704   10000.0   30.6110   142.0702  \n",
       "\n",
       "[705 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>place</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>depth</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 14:31:34.776</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.3</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>31.1148</td>\n",
       "      <td>142.2999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-31 14:23:50.014</td>\n",
       "      <td>Kyushu, Japan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>mb</td>\n",
       "      <td>27260.0</td>\n",
       "      <td>32.1049</td>\n",
       "      <td>130.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-30 23:22:38.756</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.2</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>30.9772</td>\n",
       "      <td>142.3701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-30 19:19:16.109</td>\n",
       "      <td>Off East Coast Of Honshu, Japan</td>\n",
       "      <td>4.5</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>33.1948</td>\n",
       "      <td>142.5877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-30 06:08:44.798</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.3</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>30.9180</td>\n",
       "      <td>141.8623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2021-01-05 21:02:41.764</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.1</td>\n",
       "      <td>mb</td>\n",
       "      <td>92210.0</td>\n",
       "      <td>33.0845</td>\n",
       "      <td>140.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2021-01-05 02:16:21.501</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.5</td>\n",
       "      <td>mb</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>32.3940</td>\n",
       "      <td>141.2047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2021-01-03 14:14:54.692</td>\n",
       "      <td>Eastern Sea Of Japan</td>\n",
       "      <td>4.1</td>\n",
       "      <td>mb</td>\n",
       "      <td>250430.0</td>\n",
       "      <td>43.6306</td>\n",
       "      <td>138.7964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2021-01-02 00:45:00.272</td>\n",
       "      <td>Off East Coast Of Honshu, Japan</td>\n",
       "      <td>4.6</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>33.4333</td>\n",
       "      <td>141.4589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>2021-01-01 01:50:51.107</td>\n",
       "      <td>Southeast Of Honshu, Japan</td>\n",
       "      <td>4.4</td>\n",
       "      <td>mb</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>30.6110</td>\n",
       "      <td>142.0702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>705 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time                            place  mag magType  \\\n",
       "0    2021-12-31 14:31:34.776       Southeast Of Honshu, Japan  4.3      mb   \n",
       "1    2021-12-31 14:23:50.014                    Kyushu, Japan  4.0      mb   \n",
       "2    2021-12-30 23:22:38.756       Southeast Of Honshu, Japan  4.2      mb   \n",
       "3    2021-12-30 19:19:16.109  Off East Coast Of Honshu, Japan  4.5      mb   \n",
       "4    2021-12-30 06:08:44.798       Southeast Of Honshu, Japan  4.3      mb   \n",
       "..                       ...                              ...  ...     ...   \n",
       "700  2021-01-05 21:02:41.764       Southeast Of Honshu, Japan  4.1      mb   \n",
       "701  2021-01-05 02:16:21.501       Southeast Of Honshu, Japan  4.5      mb   \n",
       "702  2021-01-03 14:14:54.692             Eastern Sea Of Japan  4.1      mb   \n",
       "703  2021-01-02 00:45:00.272  Off East Coast Of Honshu, Japan  4.6      mb   \n",
       "704  2021-01-01 01:50:51.107       Southeast Of Honshu, Japan  4.4      mb   \n",
       "\n",
       "        depth  latitude  longitude  \n",
       "0     10000.0   31.1148   142.2999  \n",
       "1     27260.0   32.1049   130.0111  \n",
       "2     10000.0   30.9772   142.3701  \n",
       "3     10000.0   33.1948   142.5877  \n",
       "4     10000.0   30.9180   141.8623  \n",
       "..        ...       ...        ...  \n",
       "700   92210.0   33.0845   140.1500  \n",
       "701   35000.0   32.3940   141.2047  \n",
       "702  250430.0   43.6306   138.7964  \n",
       "703   10000.0   33.4333   141.4589  \n",
       "704   10000.0   30.6110   142.0702  \n",
       "\n",
       "[705 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2efd9f29f17dafd1e01732ce528e1fd45416f34141c32d0e667b2a58f2e9bb04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
